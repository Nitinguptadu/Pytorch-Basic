{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 importing Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fce9c9a46d8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pdb\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 importing data in Dataset from the folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch objects, a Dataset and a DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Network Architecture of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels =1,out_channels =6,kernel_size =5)\n",
    "        self.conv2 = nn.Conv2d(in_channels =6,out_channels =12,kernel_size =5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 12*4*4, out_features = 120)\n",
    "        self.fc2 = nn.Linear(in_features = 120, out_features = 60)\n",
    "        self.out = nn.Linear(in_features= 60, out_features=10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "    # (1) input layer\n",
    "        t = t\n",
    "\n",
    "    # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "    # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "    # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "    # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "    # (6) output layer\n",
    "        t = self.out(t)\n",
    "    #t = F.softmax(t, dim=1)\n",
    "\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t\t torch.Size([6, 1, 5, 5])\n",
      "conv1.bias \t\t torch.Size([6])\n",
      "conv2.weight \t\t torch.Size([12, 6, 5, 5])\n",
      "conv2.bias \t\t torch.Size([12])\n",
      "fc1.weight \t\t torch.Size([120, 192])\n",
      "fc1.bias \t\t torch.Size([120])\n",
      "fc2.weight \t\t torch.Size([60, 120])\n",
      "fc2.bias \t\t torch.Size([60])\n",
      "out.weight \t\t torch.Size([10, 60])\n",
      "out.bias \t\t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in network.named_parameters():\n",
    "    print(name,'\\t\\t',param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 importing data in DataLoader from the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 we can pull the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 we can pull the Optimizer ( SGD )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(network.parameters(), lr=0.01)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 7 the train_loader out of the training loop cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_correct: 8898 loss: 1376.4224350452423\n",
      "epoch 1 total_correct: 28529 loss: 937.9813551902771\n",
      "epoch 2 total_correct: 39316 loss: 532.2547962665558\n",
      "epoch 3 total_correct: 42409 loss: 457.32381013035774\n",
      "epoch 4 total_correct: 43814 loss: 419.7337698638439\n",
      "epoch 5 total_correct: 44839 loss: 394.56616988778114\n",
      "epoch 6 total_correct: 45691 loss: 373.5415866971016\n",
      "epoch 7 total_correct: 46391 loss: 354.7279049754143\n",
      "epoch 8 total_correct: 47195 loss: 338.10449382662773\n",
      "epoch 9 total_correct: 47887 loss: 324.121769040823\n",
      "epoch 10 total_correct: 48445 loss: 312.1573278903961\n",
      "epoch 11 total_correct: 48861 loss: 301.9114587903023\n",
      "epoch 12 total_correct: 49215 loss: 292.7904349565506\n",
      "epoch 13 total_correct: 49536 loss: 284.59119042754173\n",
      "epoch 14 total_correct: 49852 loss: 277.26930698752403\n",
      "epoch 15 total_correct: 50108 loss: 270.6911858469248\n",
      "epoch 16 total_correct: 50340 loss: 264.81244461238384\n",
      "epoch 17 total_correct: 50543 loss: 259.31669153273106\n",
      "epoch 18 total_correct: 50735 loss: 254.09375251829624\n",
      "epoch 19 total_correct: 50933 loss: 249.1156097650528\n",
      "epoch 20 total_correct: 51131 loss: 244.5079768896103\n",
      "epoch 21 total_correct: 51310 loss: 240.0360645800829\n",
      "epoch 22 total_correct: 51473 loss: 235.84927397966385\n",
      "epoch 23 total_correct: 51564 loss: 231.84144207835197\n",
      "epoch 24 total_correct: 51756 loss: 228.15513029694557\n",
      "epoch 25 total_correct: 51840 loss: 224.72706852853298\n",
      "epoch 26 total_correct: 51949 loss: 221.49868194758892\n",
      "epoch 27 total_correct: 52059 loss: 218.47783990204334\n",
      "epoch 28 total_correct: 52170 loss: 215.60964433848858\n",
      "epoch 29 total_correct: 52273 loss: 212.87108777463436\n",
      "epoch 30 total_correct: 52365 loss: 210.32603895664215\n",
      "epoch 31 total_correct: 52460 loss: 207.95573876798153\n",
      "epoch 32 total_correct: 52553 loss: 205.6849612146616\n",
      "epoch 33 total_correct: 52612 loss: 203.47647227346897\n",
      "epoch 34 total_correct: 52682 loss: 201.46393889188766\n",
      "epoch 35 total_correct: 52761 loss: 199.46773232519627\n",
      "epoch 36 total_correct: 52816 loss: 197.59608386456966\n",
      "epoch 37 total_correct: 52905 loss: 195.79659534990788\n",
      "epoch 38 total_correct: 52949 loss: 194.12148793041706\n",
      "epoch 39 total_correct: 53009 loss: 192.51409761607647\n",
      "epoch 40 total_correct: 53086 loss: 190.8522175103426\n",
      "epoch 41 total_correct: 53141 loss: 189.30715665221214\n",
      "epoch 42 total_correct: 53192 loss: 187.77639716863632\n",
      "epoch 43 total_correct: 53223 loss: 186.26688952744007\n",
      "epoch 44 total_correct: 53265 loss: 184.90038783848286\n",
      "epoch 45 total_correct: 53324 loss: 183.49936105310917\n",
      "epoch 46 total_correct: 53349 loss: 182.2043984234333\n",
      "epoch 47 total_correct: 53384 loss: 180.9737379848957\n",
      "epoch 48 total_correct: 53446 loss: 179.7653886973858\n",
      "epoch 49 total_correct: 53488 loss: 178.46870893239975\n",
      "epoch 50 total_correct: 53536 loss: 177.40849770605564\n",
      "epoch 51 total_correct: 53616 loss: 176.25416401028633\n",
      "epoch 52 total_correct: 53669 loss: 175.0986892208457\n",
      "epoch 53 total_correct: 53716 loss: 174.09601478278637\n",
      "epoch 54 total_correct: 53729 loss: 172.9410365819931\n",
      "epoch 55 total_correct: 53763 loss: 171.91040155291557\n",
      "epoch 56 total_correct: 53803 loss: 170.7890977114439\n",
      "epoch 57 total_correct: 53856 loss: 169.79266015440226\n",
      "epoch 58 total_correct: 53879 loss: 168.80483250319958\n",
      "epoch 59 total_correct: 53924 loss: 167.80865494906902\n",
      "epoch 60 total_correct: 53978 loss: 166.8458904772997\n",
      "epoch 61 total_correct: 54010 loss: 165.88114827871323\n",
      "epoch 62 total_correct: 54033 loss: 164.9676527157426\n",
      "epoch 63 total_correct: 54084 loss: 163.99378488212824\n",
      "epoch 64 total_correct: 54108 loss: 162.98432537913322\n",
      "epoch 65 total_correct: 54137 loss: 162.00666105747223\n",
      "epoch 66 total_correct: 54164 loss: 161.1885845810175\n",
      "epoch 67 total_correct: 54206 loss: 160.19353139400482\n",
      "epoch 68 total_correct: 54225 loss: 159.463612601161\n",
      "epoch 69 total_correct: 54246 loss: 158.58484145998955\n",
      "epoch 70 total_correct: 54268 loss: 157.75211308896542\n",
      "epoch 71 total_correct: 54278 loss: 157.00947727262974\n",
      "epoch 72 total_correct: 54333 loss: 156.15149381011724\n",
      "epoch 73 total_correct: 54341 loss: 155.32196681946516\n",
      "epoch 74 total_correct: 54360 loss: 154.51745027303696\n",
      "epoch 75 total_correct: 54375 loss: 153.78232813626528\n",
      "epoch 76 total_correct: 54415 loss: 152.94329116493464\n",
      "epoch 77 total_correct: 54443 loss: 152.3165472149849\n",
      "epoch 78 total_correct: 54499 loss: 151.4144285544753\n",
      "epoch 79 total_correct: 54498 loss: 150.61284887045622\n",
      "epoch 80 total_correct: 54538 loss: 149.92886462062597\n",
      "epoch 81 total_correct: 54558 loss: 149.34574608504772\n",
      "epoch 82 total_correct: 54591 loss: 148.4607969224453\n",
      "epoch 83 total_correct: 54629 loss: 147.80114794522524\n",
      "epoch 84 total_correct: 54653 loss: 147.1074033305049\n",
      "epoch 85 total_correct: 54677 loss: 146.26835469156504\n",
      "epoch 86 total_correct: 54699 loss: 145.54773938655853\n",
      "epoch 87 total_correct: 54719 loss: 144.90864346921444\n",
      "epoch 88 total_correct: 54741 loss: 144.15886936336756\n",
      "epoch 89 total_correct: 54761 loss: 143.42639822512865\n",
      "epoch 90 total_correct: 54791 loss: 142.81475699692965\n",
      "epoch 91 total_correct: 54827 loss: 142.085624396801\n",
      "epoch 92 total_correct: 54862 loss: 141.33474116772413\n",
      "epoch 93 total_correct: 54856 loss: 140.73128195106983\n",
      "epoch 94 total_correct: 54889 loss: 140.04397216439247\n",
      "epoch 95 total_correct: 54915 loss: 139.3361513838172\n",
      "epoch 96 total_correct: 54937 loss: 138.67823518812656\n",
      "epoch 97 total_correct: 54967 loss: 137.97439896315336\n",
      "epoch 98 total_correct: 54993 loss: 137.3902292251587\n",
      "epoch 99 total_correct: 55011 loss: 136.69668041914701\n",
      "epoch 100 total_correct: 55032 loss: 136.03375497460365\n",
      "epoch 101 total_correct: 55066 loss: 135.3999786004424\n",
      "epoch 102 total_correct: 55092 loss: 134.6453666985035\n",
      "epoch 103 total_correct: 55118 loss: 133.91060846298933\n",
      "epoch 104 total_correct: 55121 loss: 133.36977311968803\n",
      "epoch 105 total_correct: 55161 loss: 132.8038932159543\n",
      "epoch 106 total_correct: 55183 loss: 132.13679586350918\n",
      "epoch 107 total_correct: 55193 loss: 131.4955333545804\n",
      "epoch 108 total_correct: 55231 loss: 130.90105739235878\n",
      "epoch 109 total_correct: 55207 loss: 130.2756651043892\n",
      "epoch 110 total_correct: 55257 loss: 129.6859470680356\n",
      "epoch 111 total_correct: 55279 loss: 128.978865288198\n",
      "epoch 112 total_correct: 55299 loss: 128.38629541546106\n",
      "epoch 113 total_correct: 55319 loss: 127.81912432983518\n",
      "epoch 114 total_correct: 55358 loss: 127.18928706645966\n",
      "epoch 115 total_correct: 55359 loss: 126.6677977219224\n",
      "epoch 116 total_correct: 55395 loss: 125.95441767573357\n",
      "epoch 117 total_correct: 55392 loss: 125.43913705274463\n",
      "epoch 118 total_correct: 55427 loss: 124.92857664451003\n",
      "epoch 119 total_correct: 55454 loss: 124.2367202565074\n",
      "epoch 120 total_correct: 55462 loss: 123.63445452600718\n",
      "epoch 121 total_correct: 55506 loss: 123.16506078466773\n",
      "epoch 122 total_correct: 55512 loss: 122.68432184308767\n",
      "epoch 123 total_correct: 55543 loss: 122.1720110476017\n",
      "epoch 124 total_correct: 55567 loss: 121.6067627929151\n",
      "epoch 125 total_correct: 55588 loss: 121.02539916336536\n",
      "epoch 126 total_correct: 55586 loss: 120.47705622762442\n",
      "epoch 127 total_correct: 55636 loss: 119.94524788111448\n",
      "epoch 128 total_correct: 55656 loss: 119.3959195278585\n",
      "epoch 129 total_correct: 55650 loss: 118.93307951837778\n",
      "epoch 130 total_correct: 55689 loss: 118.30393081903458\n",
      "epoch 131 total_correct: 55679 loss: 117.7955731973052\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(302):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch in train_loader: # Get Batch # completed 6000 image in single epoch with \n",
    "                               # batch size 100 \n",
    "        images, labels = batch \n",
    "\n",
    "        preds = network(images) # Pass Batch\n",
    "        loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "        optimizer.zero_grad() # doning zero grad value after every mini batch \n",
    "        loss.backward() # Calculate Gradients\n",
    "        optimizer.step() # Update Weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "    print(\n",
    "        \"epoch\", epoch, \n",
    "        \"total_correct:\", total_correct, \n",
    "        \"loss:\", total_loss\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Confusion Matrix with PyTorch - Neural Network Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    for batch in prediction_loader:\n",
    "        images, labels = batch\n",
    "\n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat((all_preds, preds),dim=0)\n",
    "        print(all_preds)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "    train_preds = get_all_preds(network, prediction_loader)\n",
    "    print( train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_correct = get_num_correct(train_preds, train_set.targets)\n",
    "\n",
    "print('total correct:', preds_correct)\n",
    "print('accuracy:', preds_correct / len(train_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step With new  Optimizer ( ADAM )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "  \n",
    "\n",
    "for epoch in range(40):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch in train_loader: # Get Batch # completed 6000 image in single epoch with \n",
    "                               # batch size 100 \n",
    "        images, labels = batch \n",
    "\n",
    "        preds = network(images) # Pass Batch\n",
    "        loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "        optimizer.zero_grad() # doning zero grad value after every mini batch \n",
    "        loss.backward() # Calculate Gradients\n",
    "        optimizer.step() # Update Weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "    print(\n",
    "        \"epoch\", epoch, \n",
    "        \"total_correct:\", total_correct, \n",
    "        \"loss:\", total_loss\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    for batch in prediction_loader:\n",
    "        images, labels = batch\n",
    "\n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat((all_preds, preds),dim=0)\n",
    "        print(all_preds)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "    train_preds = get_all_preds(network, prediction_loader)\n",
    "    print( train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_correct = get_num_correct(train_preds, train_set.targets)\n",
    "\n",
    "print('total correct:', preds_correct)\n",
    "print('accuracy:', preds_correct / len(train_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually Code for Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = torch.stack((train_set.targets,train_preds.argmax(dim=1)),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked[3].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt = torch.zeros(10,10, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in stacked:\n",
    "    tl, pl = p.tolist()\n",
    "    cmt[tl, pl] = cmt[tl, pl] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from resources.plotcm import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix using sk learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cm = confusion_matrix(train_set.targets, train_preds.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
