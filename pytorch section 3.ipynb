{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training with Code Example - Neural Network Programming Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f5d783107f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pdb\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n",
      "0.3.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from plotcm import plot_confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels =1,out_channels =6,kernel_size =5)\n",
    "        self.conv2 = nn.Conv2d(in_channels =6,out_channels =12,kernel_size =5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 12*4*4, out_features = 120)\n",
    "        self.fc2 = nn.Linear(in_features = 120, out_features = 60)\n",
    "        self.out = nn.Linear(in_features= 60, out_features=10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "    # (1) input layer\n",
    "        t = t\n",
    "\n",
    "    # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "    # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "    # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "    # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "    # (6) output layer\n",
    "        t = self.out(t)\n",
    "    #t = F.softmax(t, dim=1)\n",
    "\n",
    "        return t\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100)\n",
    "batch = next(iter(train_loader))\n",
    "images, labels = batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3044040203094482"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds, labels) # Calculating the loss\n",
    "\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_correct(preds,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.1908,  0.0962, -0.0513,  0.1063,  0.0319],\n",
       "          [-0.0840,  0.1050,  0.1264, -0.1102,  0.1228],\n",
       "          [-0.1420,  0.1460,  0.1768, -0.1604,  0.0758],\n",
       "          [-0.1328,  0.0490, -0.0960, -0.1086, -0.0157],\n",
       "          [ 0.1767, -0.1314, -0.0779, -0.0358,  0.1260]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1918,  0.1884, -0.1475, -0.1328, -0.1522],\n",
       "          [-0.0654,  0.1168, -0.0211, -0.0960, -0.0739],\n",
       "          [ 0.1730,  0.1818,  0.1907,  0.0132,  0.0421],\n",
       "          [ 0.1160,  0.0551,  0.0607, -0.1401,  0.0008],\n",
       "          [-0.0079,  0.0913,  0.0653,  0.0225,  0.1211]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1965,  0.1952, -0.1214, -0.0509, -0.1895],\n",
       "          [ 0.1557, -0.1298,  0.1487, -0.0408, -0.1227],\n",
       "          [ 0.1038, -0.0018, -0.1285,  0.1662, -0.0132],\n",
       "          [-0.1725,  0.0317, -0.1515, -0.0632, -0.0965],\n",
       "          [-0.0495,  0.1501, -0.1340,  0.1423,  0.1051]]],\n",
       "\n",
       "\n",
       "        [[[-0.0137, -0.1997, -0.0523,  0.0998, -0.0355],\n",
       "          [ 0.0449,  0.1636,  0.1795,  0.0786, -0.0324],\n",
       "          [ 0.1114, -0.1503, -0.0449, -0.0329,  0.1635],\n",
       "          [ 0.0532,  0.0924, -0.1029,  0.1565,  0.1169],\n",
       "          [ 0.0697,  0.1723, -0.0826, -0.0950,  0.0919]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1093, -0.0392,  0.0725, -0.1288,  0.1017],\n",
       "          [-0.0999, -0.0694, -0.0453, -0.1542, -0.1427],\n",
       "          [-0.0496,  0.0720,  0.1839,  0.0146,  0.1272],\n",
       "          [ 0.0108, -0.0419, -0.1354, -0.0653,  0.0342],\n",
       "          [-0.0435,  0.0214,  0.1935, -0.0823, -0.0169]]],\n",
       "\n",
       "\n",
       "        [[[-0.0062,  0.0783,  0.0281, -0.0188, -0.0653],\n",
       "          [ 0.0476,  0.1961, -0.0835, -0.1096,  0.1664],\n",
       "          [-0.1005,  0.0191,  0.1147, -0.1807, -0.1360],\n",
       "          [-0.0735,  0.1974, -0.1023, -0.1335,  0.0450],\n",
       "          [-0.0542, -0.1426, -0.0260,  0.1651, -0.1316]]]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(network.conv1.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.1908,  0.0962, -0.0513,  0.1063,  0.0319],\n",
       "          [-0.0840,  0.1050,  0.1264, -0.1102,  0.1228],\n",
       "          [-0.1420,  0.1460,  0.1768, -0.1604,  0.0758],\n",
       "          [-0.1328,  0.0490, -0.0960, -0.1086, -0.0157],\n",
       "          [ 0.1767, -0.1314, -0.0779, -0.0358,  0.1260]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1918,  0.1884, -0.1475, -0.1328, -0.1522],\n",
       "          [-0.0654,  0.1168, -0.0211, -0.0960, -0.0739],\n",
       "          [ 0.1730,  0.1818,  0.1907,  0.0132,  0.0421],\n",
       "          [ 0.1160,  0.0551,  0.0607, -0.1401,  0.0008],\n",
       "          [-0.0079,  0.0913,  0.0653,  0.0225,  0.1211]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1965,  0.1952, -0.1214, -0.0509, -0.1895],\n",
       "          [ 0.1557, -0.1298,  0.1487, -0.0408, -0.1227],\n",
       "          [ 0.1038, -0.0018, -0.1285,  0.1662, -0.0132],\n",
       "          [-0.1725,  0.0317, -0.1515, -0.0632, -0.0965],\n",
       "          [-0.0495,  0.1501, -0.1340,  0.1423,  0.1051]]],\n",
       "\n",
       "\n",
       "        [[[-0.0137, -0.1997, -0.0523,  0.0998, -0.0355],\n",
       "          [ 0.0449,  0.1636,  0.1795,  0.0786, -0.0324],\n",
       "          [ 0.1114, -0.1503, -0.0449, -0.0329,  0.1635],\n",
       "          [ 0.0532,  0.0924, -0.1029,  0.1565,  0.1169],\n",
       "          [ 0.0697,  0.1723, -0.0826, -0.0950,  0.0919]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1093, -0.0392,  0.0725, -0.1288,  0.1017],\n",
       "          [-0.0999, -0.0694, -0.0453, -0.1542, -0.1427],\n",
       "          [-0.0496,  0.0720,  0.1839,  0.0146,  0.1272],\n",
       "          [ 0.0108, -0.0419, -0.1354, -0.0653,  0.0342],\n",
       "          [-0.0435,  0.0214,  0.1935, -0.0823, -0.0169]]],\n",
       "\n",
       "\n",
       "        [[[-0.0062,  0.0783,  0.0281, -0.0188, -0.0653],\n",
       "          [ 0.0476,  0.1961, -0.0835, -0.1096,  0.1664],\n",
       "          [-0.1005,  0.0191,  0.1147, -0.1807, -0.1360],\n",
       "          [-0.0735,  0.1974, -0.1023, -0.1335,  0.0450],\n",
       "          [-0.0542, -0.1426, -0.0260,  0.1651, -0.1316]]]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 4.9024e-04,  6.9344e-04,  8.4223e-04,  8.3232e-04,  8.9785e-04],\n",
       "          [ 3.4066e-04,  5.0599e-04,  7.5409e-04,  9.6821e-04,  8.8736e-04],\n",
       "          [ 6.9517e-04,  6.9694e-04,  6.5502e-04,  7.7019e-04,  9.1500e-04],\n",
       "          [ 1.8306e-04,  3.1088e-04,  4.5200e-04,  5.5293e-04,  6.4700e-04],\n",
       "          [ 4.3742e-04,  3.3034e-04,  6.1228e-04,  9.9026e-04,  9.7451e-04]]],\n",
       "\n",
       "\n",
       "        [[[ 4.2756e-04,  8.1468e-04,  9.8366e-04,  1.1754e-03,  1.2567e-03],\n",
       "          [ 2.3213e-04,  7.1739e-04,  1.0464e-03,  1.2814e-03,  9.8787e-04],\n",
       "          [ 3.4908e-04,  4.7962e-04,  7.7512e-04,  9.6285e-04,  8.5428e-04],\n",
       "          [ 2.2020e-04,  3.6870e-04,  6.6762e-04,  9.6910e-04,  1.1329e-03],\n",
       "          [ 3.6153e-04,  3.1611e-04,  4.0052e-04,  7.4115e-04,  1.1938e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 2.4687e-04,  1.9379e-04,  4.4343e-05, -1.9443e-04, -2.2458e-04],\n",
       "          [ 4.7968e-04,  2.4939e-04,  3.1384e-04,  1.0714e-04, -9.8859e-05],\n",
       "          [ 5.9352e-04,  4.3288e-04,  3.8758e-04,  1.0905e-04,  4.5301e-05],\n",
       "          [ 5.6691e-04,  4.1923e-04,  1.9729e-04,  2.6755e-04,  2.3081e-04],\n",
       "          [ 3.5711e-04,  1.7562e-04,  1.0013e-05,  1.6395e-04,  2.4721e-04]]],\n",
       "\n",
       "\n",
       "        [[[ 5.5164e-05,  4.8712e-04,  3.0070e-04,  3.2850e-04,  1.7183e-04],\n",
       "          [ 3.2011e-04,  8.8378e-04,  6.5115e-04,  4.7778e-04,  3.6193e-04],\n",
       "          [ 7.5236e-04,  9.2600e-04,  6.8310e-04,  6.7520e-04,  5.7613e-04],\n",
       "          [ 5.1790e-04,  5.4205e-04,  4.5192e-04,  5.8942e-04,  9.2027e-04],\n",
       "          [ 4.1387e-04,  4.6702e-04,  2.8692e-04,  4.0904e-04,  6.1511e-04]]],\n",
       "\n",
       "\n",
       "        [[[ 6.2148e-04,  8.3997e-04,  9.4461e-04,  8.2348e-04,  8.5889e-04],\n",
       "          [ 5.2068e-04,  6.8090e-04,  7.4843e-04,  6.4894e-04,  5.8527e-04],\n",
       "          [ 3.9402e-04,  7.1303e-04,  8.8319e-04,  6.7944e-04,  6.6936e-04],\n",
       "          [ 3.8817e-04,  6.8286e-04,  9.1039e-04,  5.7060e-04,  6.2878e-04],\n",
       "          [ 4.2802e-04,  7.3525e-04,  9.6867e-04,  6.1378e-04,  6.5256e-04]]],\n",
       "\n",
       "\n",
       "        [[[-2.7987e-05, -9.9930e-05,  3.4068e-05,  3.3144e-05,  5.6834e-05],\n",
       "          [-7.9105e-05, -1.2014e-04, -3.2658e-05,  1.5262e-05,  4.4532e-05],\n",
       "          [-1.1224e-05, -4.4075e-05, -6.0747e-05,  1.1647e-05,  2.1474e-06],\n",
       "          [ 9.0939e-06, -3.6562e-05, -8.9901e-05, -4.1628e-05, -6.8355e-05],\n",
       "          [-1.8434e-05, -8.1299e-05, -9.0468e-05, -6.4811e-05, -9.3537e-05]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = network(images)\n",
    "loss.item()\n",
    "\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "\n",
    "\n",
    "get_num_correct(preds, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.1809,  0.0862, -0.0613,  0.0963,  0.0219],\n",
       "          [-0.0939,  0.0951,  0.1164, -0.1202,  0.1128],\n",
       "          [-0.1520,  0.1361,  0.1668, -0.1704,  0.0658],\n",
       "          [-0.1428,  0.0390, -0.1060, -0.1186, -0.0257],\n",
       "          [ 0.1667, -0.1414, -0.0879, -0.0458,  0.1160]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1818,  0.1784, -0.1575, -0.1428, -0.1622],\n",
       "          [-0.0754,  0.1068, -0.0311, -0.1060, -0.0839],\n",
       "          [ 0.1631,  0.1718,  0.1807,  0.0032,  0.0321],\n",
       "          [ 0.1061,  0.0451,  0.0507, -0.1501, -0.0092],\n",
       "          [-0.0179,  0.0813,  0.0553,  0.0125,  0.1111]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1865,  0.1853, -0.1314, -0.0409, -0.1795],\n",
       "          [ 0.1457, -0.1398,  0.1387, -0.0507, -0.1128],\n",
       "          [ 0.0938, -0.0118, -0.1385,  0.1562, -0.0231],\n",
       "          [-0.1825,  0.0217, -0.1615, -0.0732, -0.1065],\n",
       "          [-0.0595,  0.1401, -0.1436,  0.1323,  0.0951]]],\n",
       "\n",
       "\n",
       "        [[[-0.0237, -0.2097, -0.0623,  0.0898, -0.0455],\n",
       "          [ 0.0349,  0.1536,  0.1695,  0.0686, -0.0424],\n",
       "          [ 0.1014, -0.1603, -0.0549, -0.0429,  0.1535],\n",
       "          [ 0.0432,  0.0824, -0.1129,  0.1465,  0.1069],\n",
       "          [ 0.0597,  0.1623, -0.0926, -0.1050,  0.0819]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0993, -0.0492,  0.0625, -0.1388,  0.0917],\n",
       "          [-0.1099, -0.0794, -0.0553, -0.1642, -0.1527],\n",
       "          [-0.0596,  0.0620,  0.1739,  0.0046,  0.1172],\n",
       "          [ 0.0008, -0.0519, -0.1454, -0.0753,  0.0242],\n",
       "          [-0.0535,  0.0114,  0.1835, -0.0923, -0.0269]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0036,  0.0883,  0.0182, -0.0287, -0.0753],\n",
       "          [ 0.0576,  0.2061, -0.0736, -0.1194,  0.1564],\n",
       "          [-0.0907,  0.0290,  0.1247, -0.1904, -0.1447],\n",
       "          [-0.0832,  0.2073, -0.0923, -0.1236,  0.0549],\n",
       "          [-0.0444, -0.1326, -0.0160,  0.1750, -0.1216]]]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 2.297677516937256\n",
      "loss2: 2.273430347442627\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "batch = next(iter(train_loader)) # Get Batch\n",
    "images, labels = batch\n",
    "\n",
    "preds = network(images) # Pass Batch\n",
    "loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "loss.backward() # Calculate Gradients\n",
    "optimizer.step() # Update Weights\n",
    "\n",
    "print('loss1:', loss.item())\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "print('loss2:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training Loop Explained - Neural Network Code Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct: 47215 loss: 335.4593598395586\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "\n",
    "for batch in train_loader: # Get Batch\n",
    "    images, labels = batch \n",
    "\n",
    "    preds = network(images) # Pass Batch\n",
    "    loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() # Calculate Gradients\n",
    "    optimizer.step() # Update Weights\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    total_correct += get_num_correct(preds, labels)\n",
    "    \n",
    "print(\n",
    "    \"epoch:\", 0, \n",
    "    \"total_correct:\", total_correct, \n",
    "    \"loss:\", total_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_correct: 6005 loss: 1455.555815935135\n",
      "epoch 1 total_correct: 5990 loss: 1386.1377806663513\n",
      "epoch 2 total_correct: 5977 loss: 1386.208979845047\n",
      "epoch 3 total_correct: 5987 loss: 1386.239009141922\n",
      "epoch 4 total_correct: 5978 loss: 1386.2530558109283\n",
      "epoch 5 total_correct: 5978 loss: 1386.2599337100983\n",
      "epoch 6 total_correct: 5980 loss: 1386.2634036540985\n",
      "epoch 7 total_correct: 5980 loss: 1386.2652034759521\n",
      "epoch 8 total_correct: 5980 loss: 1386.2661182880402\n",
      "epoch 9 total_correct: 5980 loss: 1386.266589641571\n",
      "epoch 10 total_correct: 5980 loss: 1386.266829252243\n",
      "epoch 11 total_correct: 5980 loss: 1386.2669303417206\n",
      "epoch 12 total_correct: 5980 loss: 1386.2670104503632\n",
      "epoch 13 total_correct: 5980 loss: 1386.2670397758484\n",
      "epoch 14 total_correct: 5980 loss: 1386.2670521736145\n",
      "epoch 15 total_correct: 5980 loss: 1386.267074584961\n",
      "epoch 16 total_correct: 5980 loss: 1386.2670829296112\n",
      "epoch 17 total_correct: 5980 loss: 1386.2670905590057\n",
      "epoch 18 total_correct: 5980 loss: 1386.2670743465424\n",
      "epoch 19 total_correct: 5980 loss: 1386.267079591751\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(20):\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for batch in train_loader: # Get Batch\n",
    "        images, labels = batch \n",
    "\n",
    "        preds = network(images) # Pass Batch\n",
    "        loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # Calculate Gradients\n",
    "        optimizer.step() # Update Weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "    print(\n",
    "        \"epoch\", epoch, \n",
    "        \"total_correct:\", total_correct, \n",
    "        \"loss:\", total_loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
